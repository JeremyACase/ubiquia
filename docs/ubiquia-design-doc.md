# Ubiquia Design Document
This is a design document for Ubiquia, a Multi-Agent-System orchestration tool and framework.

## Document Version
- **Version:** 0.2
- **Date:** 2025-10-06
- **Author:** Jeremy Case
- **Reviewed by:** [TBD]

* [Overview](#overview)
* [Goals and Non-Goals](#goals-and-non-goals)
* [Tech Stack](#tech-stack)
* [Architecture](#architecture)
  * [High-Level Architecture Diagram](#high-level-architecture-diagram)
  * [Components](#components)
  * [Message Broker](#message-broker)
  * [Databases](#databases)
  * [Observability](#observability)
  * [Deployment](#deployment)
* [Components and Services](#components-and-services)
  * [Flow Service](#flow-service)
  * [Communication Service](#communication-service)
  * [Executive Service](#executive-service)
  * [Learning Service](#learning-service)
* [Scalability and Resilience](#scalability-and-resilience)
  * [Scalability](#scalability)
  * [Resilience](#resilience)
* [Security](#security)
  * [Authorization](#authorization)
  * [Data Protection](#data-protection)
  * [Input Validation](#input-validation)
  * [Audit Logging](#audit-logging)
  * [Compliance and Hardening](#compliance-and-hardening)
* [Observability](#observability)
  * [Metrics](#metrics)
  * [Grafana Dashboards](#grafana-dashboards)
  * [YugabyteDB Dashboards](#yugabytedb-dashboards)
  * [Logging](#logging)
* [Deployment Strategy](#deployment-strategy)
  * [Overview](#overview)
  * [Helm Structure](#helm-structure)
* [Tradeoffs](#tradeoffs)
* [Dogfooding](#dogfooding)


## Overview

**Ubiquia**  is a software system designed to serve as a general-purpose framework for the development of Multi Agent System (MAS) workflows. It is designed with lessons learned from MACHINA as well as recent advancements in MAS and Large Language Models (LLMs) such as ChatGPT and Ollama.

**Use Case Example:**
Ubiquia can be used to orchestrate a collection of SDA assets--human, LLM, hardware, or otherwise--into an autonomous system capable of surviving a military conflict while still providing invaluable real-time awareness.

---

## Goals and Non-Goals

### Goals
- Enable decentralized coordination between components across arbitrary scales
- Implement dynamic task reassignment based on current node capacity and priority.
- Enable decentralized coordination between components to optimize resource distribution.
- Provide observability into component decisions and system performance.
- Allow new components to join--or leave--dynamically without downtime.
- Require heterogenous ubiquia components to be able to realize business logic at runtime.

### Non-Goals
- Ubiquia does not handle physical device provisioning.

## Tech Stack

- **Languages**: Java (Spring Boot 3), Python
- **Database**: YugabyteDB (or H2 for testing, or resource-constrained environments)
- **Messaging**: Uses distributed database as "message broker" via inbox/outbox pattern
- **Containerization**: Docker + Kubernetes
- **Deployment**: Helm
- **Observability**: Prometheus + Grafana

## Architecture

Ubiquia's architecture is best understood through three complementary views:

1. **High-Level Architecture** — Static service layout and data flow  
2. **System Lifecycle** — How the system is installed, configured, and bootstrapped over time  
3. **Virtual Graphs** — How execution can be distributed across agents

---

### 1. High-Level Architecture

![Ubiquia Architecture](https://github.com/JeremyACase/ubiquia/blob/main/docs/diagram/system/Ubiquia_Architecture_Diagram.drawio.png)

This diagram shows the core services within a single Ubiquia Agent, their interaction with the YugabyteDB cluster, and how DAG components are executed through the Flow Service.  
It also highlights how multiple agents coordinate via the Executive Service.

---

### 2. System Lifecycle

![Ubiquia Lifecycle](https://github.com/JeremyACase/ubiquia/blob/main/docs/diagram/system/Ubiquia_Lifecycle.drawio.png)

This diagram illustrates the **deployment and runtime lifecycle** of Ubiquia.  
- **Design Time (green)** — Define Agent Communication Languages and DAGs.  
- **Provisioning Time (blue)** — Deploy to Kubernetes via Helm.  
- **Deployment Time (red)** — Bring up core services.  
- **Domain Bootstrap (orange)** — Generate schemas, entities, and domain tables dynamically.

Dashed boxes represent components generated by Ubiquia based on user-defined schemas.

---

### 3. Virtual Graphs

![Ubiquia Virtual Graph](https://github.com/JeremyACase/ubiquia/blob/main/docs/diagram/system/Ubiquia_Virtual_Graph.drawio.png)

This diagram shows how a single logical **"virtual graph"** can be distributed across multiple Ubiquia Agents.  
Each agent can host **none, some, or all** of the nodes in the graph — a property known as the **graph's cardinality** with respect to that agent.  
This enables horizontal scaling and flexible placement of computational tasks.

---

### Components
- **Flow Service**: Allows for user-defined logic to be registered, instantiated, and ochestrated in runtime.
- **Executive Service**: Coordinates tasks across a network of ubiquia components
- **Learning Service**: Responsible for updating model weights within components or even reinforcement learning across LLM components
- **Mission Logic Service**: TODO: Chat with Justin
- **Communication Service**: Exposes internal service APIs externally - dynamically (such as API's exposed in Flow Service DAG's.)

### Message Broker
- **Database**: The database will be used as a "broker" via the inbox/outbox pattern, allowing for distributed transactions with built-in "queues."

### Databases
- **YugabyteDB**: Distributed NewSQL database with eventual consistency. Works across Kubernetes clusters.
- **H2**: In-memory database for testing primarily, but also useful for short-lived belief states or resource-constrained environments.

### Observability
- **Prometheus**: Time-series database that allows for services to emit key-value-pair metadata.
- **Grafana**: Allows for importable (or customizable) dashboards to be built and visualized (primarily using Prometheus data.)

### Deployment
- **Kubernetes**: ubiquia is designed to be run in any Kubernetes environment (e.g., KIND, Rancher, etc.)
- **Helm**: Helm does double-duty as both the package manager and configuration manager for ubiquia.

## Components and Services

### Flow Service
- **Responsibilies**
  - **Agent Communication Language Registration**: The Flow Service will be able to handle "Agent Communication Language" (ACL) registration.
  - **Direct Acylic Graph Registration**: The Flow Service will be able to allow registration of Directed Acyclic Graph workflows comprised of components and adapters.
  - **Direct Acylic Graph Orchestration**: The Flow Service will double as a Kubernetes operator capable of orchestrating DAG's across Kubernetes clusters dynamically and at runtime.
  - **Schema Validation**: The Flow Service will be able to verify input/output of components based on an ACL. This is especially important for LLM-based components.
  - **Back Pressure**: Leveraging queues and the inbox/outbox pattern, Flow Service will provide a "back pressure" endpoint allowing for the Executive Service to be able to actuate Flow Service DAG's across ubiquia Agents to alleviate pressure.
  - **DAG Dataflow**: Flow Service will persist data into the distributed database via a queueing mechanism via the inbox/outbox pattern.
- **API**:
  - **ACL**: The ability to register, query, and delete ACL's
  - **DAGs**: The ability to register, deploy, teardown, query, and delete DAG's
  - **Agent Adapters**: The ability to interface with the adapter deployed for any given component depending on the type (e.g., Push, Merge, etc.)
  - **Back Pressure**: The ability to query for back pressure for any given adapter.
- **Dependencies**
  - **SQL Database**: Either H2 (for testing) or YugabyteDB.

### Communications Service
- **Responsibilies**
  - **Reverse Proxy**: The Communications Service will be able to expose services from within a ubiquia component and Kubernetes component externally, and be able to do so dynamically (such as will be needed when deploying Flow Service DAG's.)
- **API**:
  - **???**: TODO: Find a way for Comm service to get "Flow Service DAG events" (either via broker or by REST, broker complicates stuff though...) to ensure it can dynamically "surface" internal APIs via Kubernetes

### Executive Service
- **Responsibilies**
  - **Task Distribution**: Actuate Flow Service graphs across a network of ubiquia components towards optimal usage of compute resources
- **API**
  - **Leader Election**: Synchronize with other executive services across a network of ubiquia components to do leader election

### Learning Service
- **Responsibilities**
  - **Update Model Weights**: Allows for an API to update model weights for any LLM components existing in the ubiquia component

## Scalability and Resilience

### Scalability

- **Autoscaling**: Kubernetes Horizontal Pod Autoscaler (HPA) scales ubiquia core microservices based on CPU and custom Prometheus metrics (e.g., request latency, queue depth).
- **In-Transit Compute**: Executive Service will autoscale components of Flow Service DAG's across ubiquia components depending on available compute resources and edge requirements (e.g., processing large binaries via edge sensors.)
- **Database Connection Pooling**: Hikari in Spring Boot is used to manage database connection pooling to prevent overload under high concurrency.
- **Inbox/Outbox Queue Design**: Messages flowing over Flow Service DAG's can be treated as queue, where messages can be popped faster or slower depending on compute requirements.
- **Rate Limiting**: API Gateway enforces rate limits per user to protect backend services.
- **Agent Weights**: Ubiquia instances will be configurable with different "weights" (e.g., light, heavy, etc.) so that heterogenous components of a cluster can be run across anything between edge devices and the cloud.
- **DAG Cardinality**: DAGs deployed by Ubiquia agents can have cardinality configured at deployment time. This means that components and adapters within a DAG can be "toggled" on/off within a specific Ubiquia Agent.

### Resilience

- **Retry Policy**: HTTP/gRPC calls to internal services use exponential backoff with jitter and a maximum retry budget.
- **Timeouts**: All outbound requests have defined timeouts (e.g., 2s for internal calls, 5s for external).
- **Pod Disruption Budgets**: Ensures a minimum number of replicas remain available during rolling updates or node drains.
- **Operator Notifications**: Production components will leverage Grafana towards sending notifications to operators when the system changes.

## Security

### Authentication

- **JWT-Based Auth**: All external API requests require a valid JSON Web Token (JWT), issued by the centralized Auth Service.
- **Token Validation**: Middleware validates token signature, expiration, and claims before request handling.
- **Service-to-Service Auth**: Internal microservice calls use mutual TLS and signed service tokens rotated by the service mesh (Istio).

### Authorization

- **Role-Based Access Control (RBAC)**:
  - Each endpoint is annotated with access policies.
  - Roles (e.g., `user`, `admin`) are included in JWT claims and enforced by middleware.
- **Ownership Enforcement**: For resources like user profiles, services check that the caller’s `user_id` matches the resource owner.

### Data Protection

- **Encryption at Rest**: All data stored in PostgreSQL and object storage (e.g., S3) will be encrypted using AES-256.
- **Encryption in Transit**: All external and internal communication is TLS 1.3 encrypted.
- **Secrets Management**: All production secrets (e.g., API keys, DB creds) are stored in K8s secrets and injected into containers at runtime.

### Input Validation

- All incoming data is validated against strict JSON schemas.
- Server-side validation is enforced even if frontend validation is present.

### Audit Logging

- All sensitive operations (e.g., profile updates, permission changes) are logged with user ID, timestamp, and request context.
- Logs are immutable and stored in a write-only ElasticSearch index with a 90-day retention policy.

### Compliance and Hardening

- Follows OWASP Top 10 guidelines for web application security.
- Containers are scanned regularly for vulnerabilities and patched in CI/CD.
- Uses a hardened base image (Distroless or Alpine) and runs as non-root.
- Security reviews are conducted for all external API integrations and major code changes.

## Observability

Grafana is the central dashboarding and observability platform for this service. Logs, metrics, and traces are aggregated and visualized to support debugging, performance analysis, and alerting.

### Metrics

- **Exported via Prometheus-compatible endpoint** at `/actuator/prometheus` (Spring Boot).
- **Scraped by Prometheus** and visualized in Grafana dashboards.

**Key Metrics:**
TBD

**Grafana Dashboards:**
- API Latency & Throughput
- Database Query Performance
- JVM Memory & GC Metrics (if Java)
- Custom per-service KPI panels

**YugabyteDB Dashboards:**
- Latency
- Operations per Second
- Disk Storage
- Slowest queries

---

### Logging

- **Structured logs** in JSON format using Logback + Logstash encoder.
- Collected using **Elasticsearch** and visualized in **Grafana Loki**.

## Deployment Strategy

### Overview

The system will be deployed to a Kubernetes cluster using Helm as the package manager. Each microservice (e.g., Auth Service, Flow Service, Belief State) will have its own Helm chart or will be bundled into a single Helm umbrella chart for the system.

Deployments target three environments: **dev**, **staging**, and **production**. CI/CD pipelines automate deployments using Helm with GitOps-style promotion. Override values for each will be denoted, as will several "weight configurations" for a given ubiquia component (e.g., "featherweight", "lightweight", "heavyweight", etc.)

---

### Helm Structure

```text
helm/
├── Chart.yaml                  
├── values.yaml  
├── bootstrap/     
│   ├── acls/          
│   │   ├── pets-acl.json
│   │   └── workbench-acl.json
│   └── graphs/          
│       ├── pets-graph.yaml
│       └── ubiquia-workbench.yaml
├── configurations/     
│   ├── dev/          
│   │   ├── featherweight-default.yaml
│   │   ├── featherweight-dev.yaml
│   │   └── lightweight-dev.yaml
│   ├── test/
│   │   └── build-pipeline.yaml          
│   └── prod/          
│       ├── featherweight.yaml
│       ├── lightweight.yaml
│       ├── middleweight.yaml
│       ├── heavyweight.yaml
│       └── ultraweight.yaml
├── templates/
│   ├── services/
│   │   ├── core/
│   │   │   ├── flow-service/
│   │   │   ├── communications-service/
│   │   │   ├── executive-service/
│   │   │   └── learning-service/
│   │   └── test/
|   └── other/
|
```

## Tradeoffs

### Distributed Message Broker via Database (Inbox/Outbox Pattern)

**Pros:**
- Simplifies transactional consistency between message publishing and data persistence.
- Avoids introducing an external broker like Kafka or NATS, reducing ops overhead.
- Enables natural querying over message history and backpressure metrics.
- Enables treating Flow Service DAG's as queues and thus the ability to pop the queue dynanmically.

**Cons:**
- Latency is higher than with dedicated streaming systems.
- Requires careful indexing and pruning strategies for scale.
- Harder to model publish/subscribe semantics or fan-out scenarios without added complexity.

---

### Heterogeneous Agent Runtime (LLMs, Traditional Services, Edge Devices)

**Pros:**
- Flexible architecture allows orchestration of varied compute nodes (LLMs, IoT, APIs).
- DAGs can span cloud and edge environments seamlessly provided connectivity of YugabyteDB nodes into logical clusters.
- Flow Service abstraction enables runtime orchestration without recompilation.

**Cons:**
- Significant coordination overhead across diverse runtimes.
- Testing and debugging multi-type components is more complex.
- Requires detailed domain schema validation to maintain type safety across components.

---

### Helm and Kubernetes as Orchestration Backbone

**Pros:**
- Helm provides repeatable, environment-specific deployments.
- Kubernetes supports horizontal scaling and resilient runtime orchestration.
- Helm values system enables “component weight” tuning for flexible resource targeting.

**Cons:**
- Helm charts can become hard to manage at scale, especially across highly dynamic DAGs.
- Kubernetes environments vary widely (e.g., KIND vs EKS), making edge deployments tricky.
- Requires deeper Kubernetes expertise for component developers and ops teams.

---

### Use of YugabyteDB for Core State and Messaging

**Pros:**
- Global consistency model with distributed SQL semantics.
- Well-suited for inbox/outbox-based workflows and relational DAG tracing.
- Easily scales across Kubernetes clusters.

**Cons:**
- Operationally heavier than PostgreSQL; requires more resources.
- Some SQL extensions (e.g., full-text search) are limited or incompatible.
- Must manage potential cross-region latency in multi-cluster deployments.

---

### Use of H2 for Core State and Messaging

**Pros:**
- Simple, lightweight in-memory database.
- Still suports inbox/outbox-based workflows
- Loses all data when pod is torn down

**Cons:**
- Loses all data when pod is torn down
- Cannot scale past 1 pod
- Cannot be distributed across Kubernetes clusters

---

### Runtime DAG Orchestration with Flow Service

**Pros:**
- DAGs can be authored and deployed at runtime without needing redeploys.
- Enables dynamic component and adapter instantiation based on load and mission profile.
- Backpressure system allows compute-aware routing and rebalancing.

**Cons:**
- Runtime DAG orchestration increases surface area for debugging and recovery.
- Requires strong schema enforcement and introspection tooling.
- Distributed failures in a DAG may be harder to diagnose than in statically defined pipelines.

---

## Dogfooding
Dogfooding is the concept of "eating one own's cooking" in Software. For Ubiquia, this means that Ubiquia will ship with "non-core" applications themselves defined as Ubiquia DAGs. The first such DAG is the "Ubiquia Workbench."

### Ubiquia Workbench
The Ubiquia Workbench is a DAG that "ships" with Ubiquia. It has its own ACL, and once deployed, can be reached through the Ubiquia Communication Service per its configuration. The UI it ships with allows clients to request Ubiquia to use an LLM as a part of the Workbench DAG to generate a new "workflow" that gets persisted into Ubiquia. Specifically, this workflow output results in an ACL and DAG artifacts that can be used to instantiate a new belief state and workflow, respectively.  

## Contributors
* __Jeremy Case__: jeremycase@odysseyconsult.com